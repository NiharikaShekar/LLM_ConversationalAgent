// If running in Docker
ollama {
  host = "http://ollama-container:11434"
  model = "llama3.2"
  request-timeout-seconds = 500,
  range = 2
}

// If running in Docker
server {
  host = "0.0.0.0"
  port = 8080
}

// If running in Docker
yaml-logger {
  output-directory = "/llm"  # Or whatever base directory you prefer
  filename-prefix = "output-"
}

akka.http.server.parsing.max-content-length = 10m
maxWords = 100
lambdaApiGateway = "https://ax2c4dlop1.execute-api.us-east-1.amazonaws.com/CS441-HW3-main/CS441_HW3_Lambda"

// If running in local
//ollama {
//   host = "http://localhost:11434" //
//   model = "llama3.2"
//  request-timeout-seconds = 500,
//  range = 2
//}


// If running in local
//yaml-logger {
//  output-directory = "src/main/resources/Results_Conversation"  # Or whatever base directory you prefer
// filename-prefix = "Results_Conversation-"
//}


// If running in local
//server {
//host = "localhost"
//port = 8080
//}

